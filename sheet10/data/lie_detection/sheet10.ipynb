{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Optimal Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = 5\n",
    "num_sources = 10\n",
    "num_sinks = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "mass_sources = np.random.random(num_sources)\n",
    "mass_sinks = np.random.random(num_sinks)\n",
    "mass_sources /= np.sum(mass_sources)\n",
    "mass_sinks /= np.sum(mass_sinks)\n",
    "\n",
    "coords_sources = np.random.rand(num_sources, d)\n",
    "coords_sinks = np.random.rand(num_sinks, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve the OT problem as linear program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Flow matching for generative modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_checkerboard_sample(num_samples=10, field_size=0.4, num_fields=2, center=True):\n",
    "    x = torch.rand(num_samples, 2) * field_size\n",
    "    offset = torch.randint(0, num_fields, (num_samples, 2)) * field_size * 2\n",
    "    diagonal_shift = torch.randint(0, num_fields, (num_samples, 1)) * field_size\n",
    "    x += offset + diagonal_shift\n",
    "\n",
    "    if center:\n",
    "        x -= torch.mean(x, dim=0)\n",
    "\n",
    "    return x\n",
    "    \n",
    "base_distribution_std = 0.15\n",
    "num_samples = 2000\n",
    "x = torch.randn(num_samples, 2) * base_distribution_std\n",
    "y = generate_checkerboard_sample(num_samples=num_samples)\n",
    "\n",
    "# show points\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.5, label='base distribution')\n",
    "plt.scatter(y[:, 0], y[:, 1], alpha=0.5, label='checkerboard distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "from torchvision.ops import MLP\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = MLP(in_channels=2 + 1, hidden_channels=[512, 512, 512, 512, 2], activation_layer=torch.nn.SiLU)\n",
    "model.to(device)\n",
    "\n",
    "# define a loss function\n",
    "criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# train the model:\n",
    "num_epochs = 20000  # use fewer epochs if it takes too long\n",
    "batch_size = 4096\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    x = torch.randn(batch_size, 2) * base_distribution_std\n",
    "    y = generate_checkerboard_sample(num_samples=batch_size)\n",
    "\n",
    "    # TODO: implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run inference with the trained model. \n",
    "# Visualize the trajectory of the samples and the final samples at t=1.\n",
    "# Hint: Use a simple Euler integration scheme to integrate the velocity field with 100 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adversarial attacks and AI safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Connection of Tricking a Probe to Adversarial Attacks\n",
    "Adversarial attacks involve systematically perturbing inputs to deceive a model into making incorrect predictions. When a probe, designed to detect malicious behavior (like lies), is tricked, it mirrors an adversarial attack. Here, the attacker modifies the input (perturbations) so that the probe misclassifies malicious behavior (e.g., lies) as benign behavior (e.g., truth). This happens because the probe focuses on learned patterns that are sensitive to small, crafted changes in the input space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "Logistric regression in pytorch (needed for backpropagation)\n",
    "taken from https://github.com/saprmarks/geometry-of-truth/blob/main/probes.py\n",
    "'''\n",
    "\n",
    "class LRProbe(torch.nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_in, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def pred(self, x):\n",
    "        return self(x).round()\n",
    "    \n",
    "    def from_data(acts, labels, lr=0.001, weight_decay=0.1, epochs=1000, device='cpu'):\n",
    "        acts, labels = acts.to(device), labels.to(device)\n",
    "        probe = LRProbe(acts.shape[-1]).to(device)\n",
    "        \n",
    "        opt = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        for _ in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            loss = torch.nn.BCELoss()(probe(acts), labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        return probe\n",
    "\n",
    "    def __str__():\n",
    "        return \"LRProbe\"\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self.net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the DataManager class as a helper function to load the activation vectors for us.\n",
    "from utils import DataManager\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "path_to_datasets = \"/workspaces/mlph_w24/sheet10/data/lie_detection/datasets\"\n",
    "path_to_acts = \"/workspaces/mlph_w24/sheet10/data/lie_detection/acts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataManager.add_dataset() got an unexpected keyword argument 'path_to_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcities\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m dm \u001b[38;5;241m=\u001b[39m DataManager()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLlama3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_to_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_to_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m train_acts, train_labels \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m test_acts, test_labels \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataManager.add_dataset() got an unexpected keyword argument 'path_to_datasets'"
     ]
    }
   ],
   "source": [
    "# train a model on the cities dataset\n",
    "dataset_name = \"cities\"\n",
    "\n",
    "dm = DataManager()\n",
    "dm.add_dataset(dataset_name, \"Llama3\", \"8B\", \"chat\", layer=12, split=0.8, center=False,\n",
    "                device='cpu', path_to_datasets=path_to_datasets, path_to_acts=path_to_acts)\n",
    "\n",
    "# !!! Function add_dataset from given utils.py has no arguments path_to_xxx !!!\n",
    "\n",
    "train_acts, train_labels = dm.get('train')\n",
    "test_acts, test_labels = dm.get('val')\n",
    "\n",
    "print(\"train_acts.shape\", train_acts.shape)\n",
    "print(\"test_acts.shape\", test_acts.shape)\n",
    "\n",
    "# TODO: train a logistic regression probe on the train_acts and train_labels\n",
    "\n",
    "probe = LRProbe.from_data(train_acts, train_labels, lr=0.001, weight_decay=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: optimize a perturbation on a single sample which is a lie\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sample_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mtrain_labels\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m sample \u001b[38;5;241m=\u001b[39m train_acts[sample_index:sample_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m perturbation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(sample, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: optimize a perturbation on a single sample which is a lie\n",
    "\n",
    "sample_index = torch.where(train_labels == 0)[0][0]\n",
    "sample = train_acts[sample_index:sample_index + 1]\n",
    "perturbation = torch.zeros_like(sample, requires_grad=True)\n",
    "perturb_optimizer = optim.Adam([perturbation], lr=0.01)\n",
    "\n",
    "for step in range(100):\n",
    "    perturb_optimizer.zero_grad()\n",
    "    perturbed_sample = sample + perturbation\n",
    "    target_loss = nn.BCELoss()(probe(perturbed_sample), torch.tensor([1.0]))\n",
    "    target_loss.backward()\n",
    "    perturb_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: check whether this perturbation works on other samples too\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_acts\u001b[49m[train_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m perturbed_lies \u001b[38;5;241m=\u001b[39m lies \u001b[38;5;241m+\u001b[39m perturbation\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      5\u001b[0m perturbed_predictions \u001b[38;5;241m=\u001b[39m probe\u001b[38;5;241m.\u001b[39mpred(perturbed_lies)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acts' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: check whether this perturbation works on other samples too\n",
    "\n",
    "lies = train_acts[train_labels == 0]\n",
    "perturbed_lies = lies + perturbation.detach()\n",
    "perturbed_predictions = probe.pred(perturbed_lies)\n",
    "print(\"Updated predictions for lies after perturbation:\", perturbed_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perturbation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: add the constraint that the perturbation should be small\u001b[39;00m\n\u001b[1;32m      3\u001b[0m lambda_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Regularization strength\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m perturb_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam([\u001b[43mperturbation\u001b[49m], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      7\u001b[0m     perturb_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perturbation' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: add the constraint that the perturbation should be small\n",
    "\n",
    "lambda_reg = 0.1  # Regularization strength\n",
    "perturb_optimizer = optim.Adam([perturbation], lr=0.01)\n",
    "\n",
    "for step in range(100):\n",
    "    perturb_optimizer.zero_grad()\n",
    "    perturbed_sample = sample + perturbation\n",
    "    target_loss = nn.BCELoss()(probe(perturbed_sample), torch.tensor([1.0]))\n",
    "    regularization_loss = lambda_reg * torch.norm(perturbation)\n",
    "    total_loss = target_loss + regularization_loss\n",
    "    total_loss.backward()\n",
    "    perturb_optimizer.step()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
